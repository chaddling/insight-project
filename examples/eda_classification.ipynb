{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exploratory data analysis 2 </b>\n",
    "\n",
    "Having looked at some correlations and saw that regression strategy isn't working out, let's look at trying to classify the games into different sentiment classes (negative, mixed, positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime(year=2019, month=5, day=26).timestamp()\n",
    "\n",
    "# note: mean_time is in days\n",
    "cont_vars = ['num_pre','num_post','mean_time', 'release_date','len_description', 'num_movies',\n",
    "                 'num_images','score','num_reviews','sentiment']\n",
    "cat_vars = ['is_action', 'is_adventure', 'is_casual', 'is_mmo', 'is_racing', 'is_rpg', 'is_strategy', 'is_simulation', 'is_sports']\n",
    "\n",
    "data = pd.read_csv('../data/features_vec_all.csv', usecols=cont_vars+cat_vars)\n",
    "\n",
    "# convert from release date to months since release\n",
    "for i in data.index:\n",
    "    data.at[i, 'release_date'] = timedelta(seconds=int(now-data.at[i, 'release_date'])).days / 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f735a4fa358>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.index:\n",
    "\n",
    "    # games that have been out longer can have more updates\n",
    "    # -> '# updates per month since release'\n",
    "    data.at[i, 'num_post'] /= (1 + data.at[i, 'release_date'])\n",
    "\n",
    "    data.at[i, 'mean_time'] /= 30.0 # convert days->months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform and normalize the data as before\n",
    "data['num_pre'] = np.log(1 + 100*data['num_pre'])\n",
    "data['num_post'] = np.log(1 + 200*data['num_post'])\n",
    "data['mean_time'] = np.log(1 + 200*data['mean_time'])\n",
    "data['len_description'] = np.log(1+data['len_description'])\n",
    "\n",
    "data['num_pre'] = (data['num_pre'] - data['num_pre'].mean()) / data['num_pre'].std()\n",
    "data['num_post'] = (data['num_post'] - data['num_post'].mean()) / data['num_post'].std()\n",
    "data['mean_time'] = (data['mean_time'] - data['mean_time'].mean()) / data['mean_time'].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> to do: generate some visualizations </h2>\n",
    "\n",
    "I've tried some ways to visualize/split the data in features space but the results have been quite poor..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Fitting with several different regressors, as in the previous analysis. </b>\n",
    "\n",
    "We will evaluate the classifiers with f1 score: macro means unweighted average of the one-vs-rest f1 score whereas weighted means the classes are weighted by their proportions: postive ~ 0.73, mixed ~ 0.24 and negative ~0.03. The models are trained on a subset where the minority classes are oversampled.\n",
    "\n",
    "For the 3-class classification problem, we can calculate the baseline macro f1 score for i) random guess and ii) weighted guess:\n",
    "\n",
    "- If each class is guessed with $p=1/3$, then $f_{1, macro} \\simeq 0.26$.\n",
    "- For weighted guess, each class is guessed with $p_i = f_i$ where $f_i$ is the class frequency. In this case, $f_{1, macro} = 1/3 \\simeq 0.33$.\n",
    "\n",
    "In scikit-learn, f1 score offers multiclass options and seems more intuitive than AUC score when comparing against baseline classification strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 4307, 0.0: 1505, -1.0: 215})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = data[['num_post','num_pre','mean_time'] + cat_vars].values\n",
    "Y = data['sentiment'].values\n",
    "print(Counter(list(Y)))\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22 141 370]\n",
      " [ 32 194 529]\n",
      " [ 32 267 824]]\n",
      "f1 macro 0.3120218508744614\n",
      "f1 weighted 0.374965850663895\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size = 0.4)\n",
    "x_resampled, y_resampled = SMOTE().fit_resample(x_train, y_train.ravel())\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=10)\n",
    "dtc.fit(x_resampled, y_resampled)\n",
    "\n",
    "y_pred = dtc.predict(x_test)\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print('f1 macro', f1_score(y_pred, y_test.ravel(), average='macro'))\n",
    "print('f1 weighted', f1_score(y_pred, y_test.ravel(), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 50 104 339]\n",
      " [ 21 295 424]\n",
      " [ 15 203 960]]\n",
      "f1 macro 0.4247315470447888\n",
      "f1 weighted 0.4936252685017247\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size = 0.4)\n",
    "x_resampled, y_resampled = SMOTE().fit_resample(x_train, y_train.ravel())\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=15, max_depth=10, bootstrap=True)\n",
    "rfc.fit(x_resampled, y_resampled)\n",
    "\n",
    "y_pred = dtc.predict(x_test)\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print('f1 macro', f1_score(y_pred, y_test.ravel(), average='macro'))\n",
    "print('f1 weighted', f1_score(y_pred, y_test.ravel(), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  46  260  533]\n",
      " [  12   56  170]\n",
      " [  28  286 1020]]\n",
      "f1 macro 0.30003789854624\n",
      "f1 weighted 0.4169995455065479\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size = 0.4)\n",
    "x_resampled, y_resampled = SMOTE().fit_resample(x_train, y_train.ravel())\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=100.0, solver='lbfgs',multi_class='ovr')\n",
    "log_reg.fit(x_resampled, y_resampled)\n",
    "\n",
    "y_pred = log_reg.predict(x_test)\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print('f1 macro', f1_score(y_pred, y_test.ravel(), average='macro'))\n",
    "print('f1 weighted', f1_score(y_pred, y_test.ravel(), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "def get_mean_f1(skf, X, Y, model):\n",
    "\n",
    "    indices = skf.split(X, Y)\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_indices, test_indices in indices:\n",
    "        \n",
    "        xr_train, xr_test = X[train_indices], X[test_indices]\n",
    "        yr_train, yr_test = Y[train_indices], Y[test_indices]\n",
    "    \n",
    "        xr_resampled, yr_resampled = SMOTE().fit_resample(xr_train, yr_train.ravel())\n",
    "        model.fit(xr_resampled, yr_resampled)\n",
    "        yr_pred = model.predict(xr_test)\n",
    "    \n",
    "        f1_scores.append(f1_score(yr_pred, yr_test, average='macro'))\n",
    "\n",
    "    return np.mean(f1_scores), np.std(f1_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the models on their scores using 10-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3590218011684674, 0.03246725827227365)\n",
      "(0.3804666332418735, 0.020702495887429503)\n",
      "(0.3443918449484222, 0.03435557696548091)\n"
     ]
    }
   ],
   "source": [
    "print(get_mean_f1(skf, X, Y, dtc))\n",
    "print(get_mean_f1(skf, X, Y, rfc))\n",
    "print(get_mean_f1(skf, X, Y, log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> In the end, although not better by much than weighted guess, the random forest classifier outperforms others. Adding the categorial features also helps - it does split the feature space up in other directions which makes the classifier perform better than the weighted guess baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
